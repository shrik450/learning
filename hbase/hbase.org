#+TITLE: HBase Training Notes
* Intro
  - Hbase is a NoSQL database for big data.
  - Its storage architecture is very similar to other NoSQL dbs
  - Hbase is a key-value raw store
  - Use Phoenix on top of HBase as a more egonomic access layer.
* NoSQL
  - Common setup: relational + cache
  - Locks are expensive, this caused spikes to tank performance
  - Two companies which ushered in NoSQL: Google (via BigTable,
    Dremel, ZFS (?)) and Amazon (DynamoDB)
  - HBase is OSS BigTable, Cassandra is OSS Dynamo
** Issues with common setup
   - Not designed to be scaled
   - Locks are expensive and reading needs locks
   - Sharding
** NoSQL & Why
   - Non relational
   - Relax some of ACID CAP theorem coming up
   - Dynamic langs and dynamic schema
   - Revolutionary for the time sotrage arch
   - Large datasets + acceptance of dynamic types
** Storage Arch
   - Random IO is the bottleneck for RDBMS
   - Disk read/write ahead optimizes sequential reads/writes
   - Random IO requires seeks which are 10^2 slower than sequential
     read
   - Random IO pays the latency of the disk every time
   - Locking is prohibitivelye expensive
   - b trees are bad for writes
*** Lock-free Logs - CAS (Compare - and - Set)
    - =AtomicInteger= in Java for lock-free changes
    - This is like CRDTs
    - =CAS(memadd, exv, newv)= - if val at =memadd= is =exv= change it
      to =newv=. =cmpxchg=
    - This is hardware backend and cheaper than a software lock.
    - "spinning" - retrying after failure to update due to CAS. Still
      faster than locking.
    - Window times ~25-30ns for the instruction (compare w/ ~100-200ms
      for OS)
*** XADD - Wait free
    - this instruction takes only a \Delta change
    - This just adds and returns the previous value
    - order of 3-4x faster than even CAS.
*** Log File
**** Memory mapped file (mmap)
     - Normal files: Heap -> JVM Heap -> Buffer -> Disk
     - Mmap: Direct to the Buffer.
**** Wlog
     - An atomic long controls where the write happens
     - =wlog_entry(e)= -> gives you the address of where to write and
       leaves the required length available to you. This is basically
       just a counter.
     - Essentially, an array you ask for space in, and using XADD it
       always gives you space that's guaranteed free.
     - This solves /both/ locks and random writes - all writes are
       sequential after all!
     - The log isn't the be-all, end-all - the data can't be searched
       etc.
*** Storage
    - In Memory Skiplist Map (IMSM)
    - WAL is only for order of arrival handling
**** Regular DB
     - B+ Tree in disk
     - Each branch has multiple items
     - Binary tree in disk leads to way too many seeks, hence B+.
     - O(nlogm) - n fanout, m total size / fanout (BUT ONLY WHEN
       WRITING)
     - Writing requires acquiring lock -> seek to pos -> edit ->
       rebalance, and now lock timings dominate.
     - Two problems: B+ writes, random writes, bigger than memory
       file.
**** NoSQL
***** Skip List Intro
      - Binary tree, but not a regular old binary tree.
      - No-lock b tree (lol)
      - If I were a RDBMS author, I would simply use a no-lock b tree.
      - It's a trie? No I misunderstand tries.
      - roughly half splits for locations
      - Use CAS for lock free changes but requires retries.
      - Consider a link list with more link lists above it that
        stategically point to certain points at roughly half splits.
      - 1 -> 2 -> 3 -> 4 -> 5, adding 4.3 and 4.7
***** Reindexing
      - Each thread does a random check, if true it adds to the upper
        index, otherwise it doesn't.
      - It maintains the "roughly half" structure.
      - Very wait free
      - =ConcurrentSkipListMap=, doesn't need a lock!
      - Why not HashMap? Need to support point to point query
        i.e. ranges and the like.
*** The full picture
    - First add an incoming row to the WAL by breaking it down into cells
    - Each property being changed takes one cell =id:field:timestamp:value=
    - These cells are then moveed into the IMSM, where it gets sorted
      and is ready to sort.
    - The IMSM is stored into the disk when it becomes full.
    - Updates are just stored into the IMSM and then reconciled.
    - System crashes are recovered by replaying the WAL.
    - You can configure HBase to handle WAL -> IMSM async for higher
      throughput / lower latency.
**** Cell Style Storage
     - As mentioned in [[*The full picture][The full picture]], data is stored in both the
       WAL and the IMSM in cells, not as rows or recrds.
     - The IMSM in memory is called the "mem store".
     - Check the IMSM for the key, then in reverse order of TS.
     - This ensures you don't have to take locks to update data.
     - The disk storage is called HFile.
** Cluster Architecture
   - Master - region servers.
   - Region servers are on different physical nodes.
   - Each "table" is split across different regions, with each region
     having a shard.
   - There's an index for the distributed data, that lists the
     start:end range of keys on each shard.
   - Each region servers has different regions with each having a mem
     store for the entire column family with multiple store HFiles.
   - There's a single WAL for the entire region server.
   - Zookeper holds the distributed index.
   - The region server holds the memstores
   - The Data node holds the WAL, stores and files
   - Region Server + Data Node = Physical Node
** CAP
   - Consistency - Availability - Partition Tolerance: Pick Two
   - CA: RDBMS
   - AP: Cassandra, Dynamo, Riak
   - CP: Hbase, Mongo, Redis
   - My database is consistent, available and partition tolerant - no
     cap ;)
*** HBase's CAP tradeoff
    - HBase gives up on Always Availability for Consistency.
    - HBase leaves durability of the disk data to the HDFS system.
    - HDFS data is replicated (usually) thrice.
    - If a region server goes down, those keys will become unavailable
      until a new region server pops up and serves that data.
    - There's no redundant storage within HBase itself to maintain
      consistency.
      - Serving writes from two nodes at once could lead to
        inconsitent data until reconciliation.
*** Cassandra
    - Cassandra stores data based on a replication factor in multiple
      nodes w/ a peer2peer arch.
    - If multiple writes come in, the node can try to reconcile, but
      the answer may never be consistent as a write can come in during
      reconciliation.
    - Graceful degradation of performance if nodes go down, but
      reads/writes are always present for all keys.
    - Even if a node doesn't have data about a key, it can accept a
      write req and passes it on to a node that does.
    - Developed at FB.
** MVCC locks
   - Row level locks when multiple agents are trying to update the
     same row.
   - Multi version concurrency control
   - Obtain row lock -> write to WAL -> Update memstore -> release row lock.
   - Doesn't affect reads.
   - Keeps a write number which is updated when lock is release
   - Reads always pick up the latest complete write number.
* Data Model
  - BigTable is sparse - keys can be sparse due to the cell storage.
  - Indexed by row x column x timestamp
  - Master/Slave architecture, with the master node as a single point
    of failure.
    - The master handles crashes and automatic partitioning. Not used
      for r/w at all.
    - HBase3: Replace Zookeper with the master.
  - Sine tables are across files, joins are especially bad for perf.
** Column Families
   - The more columns you have, the more cells you have to traverse
     per row, so you should minimize the number of columns.
   - You can optimize by splitting columns across files using "column
     families" - if the majority of your queries use 5/10 columns
     splitting them into two files can improve perf. (Data oriented
     design, wow)
   - Columns can be added/removed without changing the schema because
     it's just about adding cells.
* HBase Operations internals
** Scanner
   - =cellScanner()= gives the row as cells.
   - HBase only ever stores things as bytes.
   #+begin_src java
     Put put = new Put(Bytes.toBytes("testRow"));
     put.addColumn(Bytes.toBytes("fam-1"), Bytes.toBytes("qual-1"),
                   Bytes.toBytes("val-1"));
     //                          family                   col
     put.addColumn(Bytes.toBytes("fam-1"), Bytes.toBytes("qual-2"),
                   Bytes.toBytes("val-2"));
     //                          value
     CellScanner scanner = put.cellScanner();
     while (scanner.advance()) {
         Cell cell = scanner.current();
         System.out.println("Cell: " + cell);
     }
   #+end_src
   - Column Family has to be specified at creation and used for every
     insert
** Data Versioning
   - When creating a table, you can specify the number of versions to
     keep
     #+begin_src sql
       create 'test', { NAME => 'cf1', VERSIONS => 3 }
       put 'test', 'row1', 'cf1', 'val1'
       put 'test', 'row1', 'cf1', 'val2'
       scan 'test', { VERSIONS => 3 }
       -- row1 value=val2;
       -- row2 value=val1;
     #+end_src
** Client Connections
   - When the colume of writes is v. high, you can skip the memstores.
   - The =Connection= class isn't threadsafe, you should create a new
     connection for each thread.
** Buffered Mutations
   - Avoid making one RPC per mutation, use a =BufferedMutator= instead.
   - =Put= -> one RPC per mutation.
   - =PutList=
     - Batches but does not sort internally
     - The issue here is that you have to maintain connections to all
       region servers because your mutations can hit all of them.
   - =BufferedMutator=
     - The buffered mutator does a local sort on the client based on
       the global index data it gets from the master, and then hits
       the right region server for each set of puts that require that
       RS.
     - So it hits each region server only when required.
     - This is the optimal way to mutate.
     - If any of them fails, gives you an exception specifically for
       the ones which failed.
** Atomics
   - =checkAndPut= - makes a put if the check passes atomically.
   - What about upserts?
   - I just realized that a put by default should be an upsert, it
     shouldn't actually check if the row is not present.
   - =checkAndMutate=
** Useful Methods
   - max results per column family
   - set / get nearest before
   - append to same value
* More on the Storage
  - "If we're storing across files and the memstore, do we need to go
    through all of them to get a row?"
** Sequence Files
   - A binary file format
   - Header - comparison, keytype, valuetype
   - Sync - marks the start of a block of variable length
   - Block
     - Number of records
     - Compressed length
     - Compressed keys
     - Compressed value length
     - Compressed values
   - Header - Sync - Block - Sync - Block - Sync ...
   - Compressed keyvalue store, essentially. Compression ratio on keys
     generally higher than on values.
** Map File
   - Two sequence files: Index and Data file.
   - Both files are sorted by key
   - Index files stores the key to block id in data file
   - Index file can be cached in mem all the time.
   - Bin search on Index in memory -> seek to the right sequential
     blocks in disk on the data file. (they keys are sorted so your
     requirements are either range queries or id queries)
   - HFile has the index and the data in the same file.
** Caching
   - Each Region Server has a block cache which reatins recentlya
     ccessed data for contiguous info.
   - =setCacheBlocks= and =getCacheBlocks= to control the caching
   - not a silver bullet, for random reads avoid cache churn
** Scans and Filters
   - Scan checks the entire range starting after your start and until
     your end
   - Filter scans and then removes data that matches; will still read
     every row
   - =ComparisonFilter= to keep rows that match
   - There's a lot of filters.
   - Covered Indexes: an index where all the required information is
     in the key. Use a =KeyOnlyFilter= to scan for these.
     - The key only filter can also return the length as the value.
* Phoenix
  - Not the elixir one; Apache Phoenix.dd
  - The phoenix shell is an SQL wrapper over HBase;
  - Designed by salesforce, huh
  - Sql -> Phoenix -> HBase API calls like Put, Get, Scan etc.
  - The phoenix client plans the query, hits the HBase API, which then
    hits a Phoenix coprocessor on each region server before reaching
    into HDFS.
    - The coprocessor is necessary for the things like secondary
      indexes that Phoenix supports.
  - Supports:
    1. Transactions* (shouldn't be using HBase directly elsewhere)
    2. UDFs
    3. Some other stuff
    4. Secondary indexes (not ACID compliant)
    5. Views* (some limitations)
    6. Constraints (=NOT NULL=) - only on pkey.
  - Phoenix Query Server: allows clients not speaking java to use an
    intermediary server to run Phoenix queries.
  - NEVER JOIN on phoenix. Since the data is sharded, joins will
    easily spill over region servers and have insane latency.
** Secondary Indexes
   - Useful when you can't just shove every access pattern into the
     key
   - Skip scans: I didn't get this
** Reading
   - Plans the query based on available knowledge of indexes
   - Finds optimal plan
   - executes scans  parallelly using an =ExecutorService= pool
   - Results are merged on the client, which can lead to OOM in some
     cases on the client.
   - Writes are just converted into Put (didn't need a new section for
     this)
** Salted Tables
   - Consider the hotspotting case as discussed in [[*Use Cases][Use Cases]] - Time
     series data.
   - The effect seems to have been to spread the rows across regions.
   - Not always necessary; it affects your read speeds.
   - You can add a rowkeyorder config to make phoenix sort, because
     the order is perturbed by the salting.
   - This is not always better. Do it only if:
     1. Your writes are hotspotting because of a monotonically
        increasing key;
     2. That's a problem, because the server is getting overwhelmed;
        and
     3. The data is not read sequentially, as that is better served by
        keeping close data in the same region server.
   - Salting: Prepend two bytes of the hash to the key.
   - Bucketing: Create n paritions of the data. This is fixed -
     changing it involves rehashing everything.
   - Salt then bucket: key gets perturbed for more even utilization of
     region servers.
   - No. of buckets != No. of region servers, buckets are spread
     across servers
   - Not "consistent hashing"
   - Phoenix does the sorting, and transparently adds and removes the
     prefix from the key.
** Column Families
   - Phoenix support for HBase Col Families.
   - Again, only for specific purposes.
   - Data oriented, not design oriented.
** Good Ol' SQL stuff
   - =WHERE=, =HAVING=, =JOIN= and =ORDER BY= are all supported but they all
     have costs associated due to the data/cluster arch.
   - Bucketed SortMerge joins vs Broadcast Joins
   - You can hint to the engine which type of join you want
   - EXPLAIN to ginf what it goes for.
*** Joins, Streaming
    - Also called a Sort Merge Join
    - Done using MapRed
    - Shuffle and Sort via MapReduce - expensive.
    - Runs a map on every region server with the data to generate a
      key-value, where the key is (id, table)
    - 60% of mem in the RS has to be reserved for the MR
*** Broadcast Join / Hash Join
    - Look at the smaller side and combine it in one place.
    - If it fits inside memory, broadcast it to all systems
    - Doesn't work if both sides are large.
    - The other side is calculated from the table in memory.
** Sparking Phoenix
   - HBase for pure storage and Spark for pure compute
   - Also useful as you can OOM from client side merges if you use a
     plain java client, better merge in Spark instead.
   - Still uses phoenix, but only on the HBase server side.
*** Joins in Spark and Phoenix
**** Pagerank example
     - Process:
       1. Each page has initial rank 1
       2. Each page contributes rank / |neighbors| to its neighbors
       3. Set rank to 0.15 * 0.85 * contrib
       4. Loop from 2 until stability.
     - Naturally, over a large dataset like the entire web, this has
       to be optimized to work well.
**** Spark Joins
     - Structure:
       - Rank is one side/table
       - Link is one side/table
     - Spark can colocate and copartition the keys w/ special
       operators - =mapValues=

       (See: Spark Internals-2.pdf, slides 33-)
     - This pushes shuffles to be run only when required.
     - Practically worse on the lab system :)
**** More
     - By default, Phx+Spark still goes for a broadcast join if
       possible (it's faster than the spark join)
       - You can configure using the broadcast join threshold.
     - You can also save the intermediary tables as Spark managed
       tables
** More Join stuff
   - Semi/anti joins are automatically used when it's optimal (like
     for =EXISTS= clauses)
** Secondary Indexes
   - You can't just make a good ol' B+ tree like on RDBMS because the
     data is spread out.
   - So you have an index table on HBase that's also a distributed.
   - Phoenix keeps the secondary index in sync - this adds a cost to
     updates.
     - The coprocessor allows hooks into WAL events which allows the
       framework to queue up index changes.
   - Making a table append-only makes it cheaper to index.
   - The phoenix query planner automatically uses indexes when
     available.
   - Spark doesn't always work well with phoenix indexes and you have
     to work around that.
     - Generally it's supposed to /push the predicate down/.
     - Doesn't happen with timestamps - convert to a number instead.
     - This is a bug, not intentional.
   - Mongo benchmarks indexes to find out which one is faster for a
     search.
   - You can also make functional indexes.
*** Global & Local Index tables
    - For each index, you'll be making a write to update it.
    - "global" indexes - default - indexes are across the whole table -
      makes reading easy.
    - Local indexes maintain a region-wise index
* Use Cases
  - Search Indexes
  - Time series data
    - If the key is just the timestamp, it hotspots the writes into
      one region server because sharding is by key
    - So the key should be prefixed by something else to distribute
      the writes more evenly.
* Case Studies
** Image Search
*** Text Search
    - Works by reverse indexing
    - Tokenization and stop word removal
    - Lucene (I always mix this up with Lucerne)
    - ES: Multiple lucene
    - Lucene + Hbase: Lucene indexes into HBase - distributed store +
      fast access.
    - Sample key: =contents|boat= <- allows searching by "content:
      boat" super quickly.
*** Onto Images
    - Calculate pattern of image and store it in the key for quick access
    - The file itself can be sorted in HDFS, with another index that
      keeps track of the position.
    - Since the seqfile is essentially a key - value store, you don't
      need to store position and offset - you can find the image by
      key.
** As a Centralized Lookup Store
   - As a replacement for Redis
   - <1GB Data
   - Shouldn't use HBase for this.
   - But what if we want to avoid adding a Redis data store? We
     already have an HBase installation.
   - Most of the time HBase will work fine, but there could be latency
     spikes from occassional issues.
   - Depends on your target latency percentile. 0.1% of the time if
     the latency is ~1s and that's not OK, don't use HBase
   - Ex: if there's a GC happening/ region or master crash.
** OpenTSDB
   - Data is sorted by timestamp all the time.
   - Used for metrics and observability hence the key /has/ to be
     sorted.
   - Hotspotting is a major concern.
   - Only one table and only one Colfamn.
   - Their key: |metric|time|tagk|tagv|
   - Time normalized over the hour.
   - This leads to lots of metrics for the same hour stored in the
     same row.
* Tuning
** Regions/store
  - HBase is meant to run with a small number of regions (20-200) per
    server.
  - This is partly because 2MB of RAM is used per region memstore
    for metadata, so with ~1k memstores you're already hitting
    gigabytes of wasted RAM.
  - Plus, if each memstore has only a few MB of memory you're
    spilling to disk all the time.
  - This is also hard for the master to keep up with as it gets heavy
    on the zookeeper storage.
** Memory
   - Compute + Storage if you're not using spark
   - If using spark, almost purely for storage.
   - Formula for allocation:
   #+begin_quote
   RegionSize/MemstoreSize * ReplicationFactor * HeapFraction for
   memstores
   #+end_quote
   - So for 10T of raw disk you need to allocate ~107GB. (3.3Tb
     effective, 10GB regions ~338 regions. 128 * 338 / 0.4)
** ColFams
   - 2-3 column fam is about the most.
   - Flushing is on a per region basis -> neighboring column families
     also get spilled.
   - So cardinality also has to be evenly balanced, as a high
     cardinality family will make mass scans on other colfams slower.
** Keys and Hotspotting
   Covered in [[*OpenTSDB][OpenTSDB]].
   - Column names should be short as well.
   - Manual splits have to be careful. Consider string keys, if you
     split based on hex values large gaps can appear in them.
     - If your strings store hex data in them, the vast majority of
       the string space can't appear, and HBase's partitioning will
       hotspot.
   - Tall v Wide v Middle
     - Rows or versioning? Preference: Rows. Keep the timestamp in the
       rowkey, esp. if there's a lot of versions.
** Latency
   - Normal RTT is ~ms based on etwork RTT _ disk speeds.
   - Client retries can occur when HBase is rebalancing data.
   - GC latency  - ~50ms most times, 2s at most.
   - Server loss latency - ~minutes
   - Writes overwhelming the memstore
   - Data locality issues - no longer matters as much as network
     speeds and latencies are pretty low.
     - Cache locality still matters.
* Stray Notes
  - Compacting to remove duplicates in the data. This doesn't block as
    much as possible because reads can go on unhindered from the
    existing files while writes go to the memstore.
  - Deletes are via tombstones.
  - The storage arch is backed by hdfs, which can have bad latency,
    but the actual bottleneck is the memstorea, and you need to
    configure it appropriately to avoid running into hdfs limits.
  - HBase vs Cassandra comes down to HBase's usage of HDFS as the
    generic distribute filesystem, while Cassandra takes care of
    replication itself. Cassandra can be tuned to be Consistent.
  - Only the column families has to be predefined, actual columns can
    be added whenever and as you want.
  - Cassandra natively supports secondary indexes :thinking:
  - =hbase htop= : top but for hbase.
  - When you need extremely low latency, you should use off-heap
    memory - the GC doesn't know about it.
